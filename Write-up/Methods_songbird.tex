In this paper, we used the simulated Integrate and Burst (IB) model introduced in \cite{Fiete}. We then implemented STDP learning and Hebbian learning.

\subsection{Integrate and Burst Model}

Our IB model is an implementation of equations described in \cite{Fiete}. We provide a detailed description of those equations here, with more attention paid to the intuition behind these equations. There are \(N\) neurons connected in an all-to-all environment. Each neuron \(i\) bursts when its membrane potential, \(V_i\), hits the threshold \(V_\theta\). We assume that every neuron bursts for \(T_{burst}\) time. While bursting, each neuron fires four times uniformly over the burst interval before resetting to \(V_{reset}\).

When a neuron is not bursting, its potential is governed by a typical conductance based leaky integrate and fire model with built-in inhibition:

\begin{equation}
\tau_V \od{V_i}{t} = -g^L(V - V^L) - g^E_i(V - V^E) - g^I_i(V - V^I)
\label{Potential}
\end{equation}

The leak conductance, \(g^L\), is assumed to be a homogeneous constant input, as is the leak potential \(V^L\). Notice that in the absence of excitatory or inhibitory conductance, neuron \(i\) will tend to \(V^L\). Therefore we can say that the leak potential is also equal to the rest potential.

The excitation potential, \(V^E\), and the inhibition potential, \(V^I\), act as upper and lower bounds on untethered potential respectively. 

The excitation conductance, \(g^L\), is defined by the activity in neighboring neurons as well by external random stimulation:

\[g^L = Ws + W_0b\]

Where \(W_{ij}\) is the strength of the synapse from neuron \(j\) to neuron \(i\), \(W_0\) is the conductance strength of external synapses and \(b\) is a Poisson random variable with frequency \(r_{in}\). \(r_{in}\) was an input parameter. In \cite{Fiete} \(r_{in}\) was constant, but we chose to assign \(r_{in}\) a high value and anneal it to jump-start neural activity. \(s_i\) is the activation of neuron \(i\). It is incremented each time neuron \(i\) fires, and it decays by

\[\od{s}{t} = -\tau s\]

The inhibitory conductance, \(g^I\), is defined as the sum of the adaptation inhibition, \(g^I_{ada}\), and the global inhibition, \(g^I_{glob}\). The adaptation inhibition is the internal inhibition generated by activation of a neuron. It is defined by the same equations as \(s_i\), but with a constant multiplier \(A_a\) and a slower time-constant \(\tau_{ada}\).

\subsection{Learning}

The purpose of this paper is to replicate and test some of the claims made in \cite{Fiete}. One of the claims made is that the model used by \cite{Fiete} demonstrates how STDP can contribute to the synchronous regular synfiring chains exhibited in Zebra Finch \cite{Hahnloser2002}. We implemented STDP, but we also implemented a pure Hebbian learning rule to demonstrate that STDP learning is not necessary for the model to exhibit synchronous regular firing chains.

\subsubsection{STDP Learning}

We followed \cite{Fiete} precisely in our implementation of STDP learning. Define

\[K(t) = 
\begin{cases}
e^{-t/\tau_{STDP}} &\text{ if } t > 0\\
-e^{-t/\tau{STDP}} &\text{ if } t < 0\\
0 &\text{ otherwise}
\end{cases}
\]

as the STDP kernel. For every pair of neurons \(i,j\), let \(t_i < t_j\) be two times when \(i\) and \(j\) fired respectively. Then According to STDP, the weight matrix element \(W_{ij}\) should increase proportional to \(K(t_j - t_i)\). Notice that the change in \(W\) according to STDP is approximately anti-symmetric. \cite{Fiete} strengthened the nonlinearity of STDP by making STDP growth of a synapse proportional to the strength of that synapse (with an added factor so 0 weight synapses could grow). In particular, the paper defined:

\begin{equation}
\Delta^{STDP}_{ij}(t) = \left(\frac{W_{ij}(t-1)}{w_{max}} + 0.001\right)*\left(x_i(t)K(0)x_j(t) + \sum_{\tau = 0}^t  [x_i(t)K(\tau)x_j(t -\tau) - x_i(t - \tau)K(\tau)x_j(t)]\right)
\label{STDP}
\end{equation}

Where \(x_i(t)\) is a binary variable taking the value 1 if neuron \(i\) fired at time \(t\). So, at each time \(t\), \(\Delta^{STDP}(t)\) is proportional to the change in \(W\) with respect to STDP. Our implementation assumed \(K(t) \approx 0\) for \(t > 4\)ms.

However, the key innovation of this paper was to introduce a secondary source of competition. If the total strength of all synapses into or out of a given neuron exceed a soft limit \(W_{max}\) after STDP learning, then all such synapses experience heterosynaptic long-term depression (hLTD) proportional to the amount by which the soft limit is exceeded:

\begin{equation}
\theta^{col}_i =  \left[\sum_{j=1}^N (W_{ij} + \eta\Delta^{STDP}_{ij}) - W_{max}\right]^+
\label{hLTD}
\end{equation}

\(\theta^{col}\) denotes the amount by which the soft limit has been exceeded. Define \(\theta^{row}\) similarly. Then,

\begin{equation}
\od{W_{ij}}{t} = \eta\Delta^{STDP}_{ij} - \epsilon(\theta^{col}_i + \theta^{row}_j)
\label{Learning}
\end{equation}

This updates weights according to STDP along with a penalty if the soft limit is exceeded. Finally, to ensure that STDP never diverges, we implement a hard limit. If any element \(W_{ij}(t) > w_{max}\), where \(w_{max}\) is a constant parameter, we manually set \(W_{ij}(t)\) to \(w_{max}\). 

We actually implemented the results incorrectly based on a typo in \cite{Fiete}. Our implementation used the following equations:

\begin{equation}
\theta^{col}_i =  \left[\sum_{j=1}^N (W_{ij} + \Delta^{STDP}_{ij}) - W_{max}\right]^+
\label{hLTD-wrong}
\end{equation}

\begin{equation}
\od{W_{ij}}{t} = \eta\Delta^{STDP}_{ij} - \eta\epsilon(\theta^{col}_i + \theta^{row}_j)
\label{Learning-wrong}
\end{equation}

As demonstrated in sections 3 and 4, this causes some significant issues with convergence. Unfortunately, we noticed the problem too late into the project to redo the simulations.

\subsubsection{Hebbian Learning}

Hebbian plasticity is a general family of learning rules which strengthen synapses between neurons which fire with high correlation. Hebbian learning rules are generally considered incomplete because while they explain how synapses are strengthened, they do not explain how synapses might be weakened. 

One way to complete the Hebbian learning rule is with STDP. In STDP learning, highly correlated neurons experience a large change in synaptic strength, but the direction of the change (increase or decrease) depends on the relative timing of firing between the synapses.

However, the soft and hard limits imposed by the model also provide a way for synapse strength to decrease, so it should be possible to implement this model using Hebbian plasticity. To do this, we ran the exact same learning algorithm as implemented for STDP, but we modified the kernel to be

\[K(t) = 
\begin{cases}
e^{-t/\tau_{Heb}} &\text{ if } t > 0\\
0 &\text{ otherwise}
\end{cases}
\]

With this new kernel, 

\begin{equation}
\Delta^{Heb}_{ij}(t) = \left(\frac{W_{ij}(t-1)}{w_{max}} + 0.001\right)*\left(x_i(t)K(0)x_j(t) + \sum_{\tau = 0}^t  x_i(t)K(\tau)x_j(t -\tau)\right)
\label{Heb}
\end{equation}

gives us a Hebbian learning rule. We implemented the soft and hard synaptic weight limits as in STDP learning.

\subsection{Parameter Choices}

Our implementation of the IB neuron network with STDP learning matched that used in \cite{Fiete} in many cases. However a few parameters were chosen differently. The table below provides a list of parameters we used:

\begin{center}
\begin{tabular}{|c|c|p{0.5cm}|c|c|p{0.5 cm}|c|c|}
\cline{1-2}\cline{4-5}\cline{7-8}
Parameters & Values & & Parameters & Values& & Parameters& Values\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(dt\) & \(2 \times 10^{-5}\)s& & \(\tau_V\) & 0.01 F/m\(^2\)& &\(V^L\) & -0.06V\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(V^E\) & 0V& & \(V^I\) & -0.07V& & \(W_0\) & 5S/m\(^2\)\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(V_{\theta}\) & -0.05V & & \(V_{reset}\) & -0.055V & & \(T_{burst}\) & 0.006s\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(\tau\) & 0.004s & & \(r_{in}^{start}\) & 10000Hz & & \(r_{in}^{min}\) & 4000Hz, 6000Hz\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(N\) & 50 & & \(w_{max}\) & 0.14 & & \(W_{max}\) & 0.14\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(\eta\) & varies & & \(\epsilon\) & varies & & \(A_g\) & 4\\

\cline{1-2}\cline{4-5}\cline{7-8}
\(A_a\) & 9 & & \(\tau_{STDP}= \tau_{Heb}\) & 0.02s & & \(\tau_{ada}\) & 0.015s\\
\cline{1-2}\cline{4-5}\cline{7-8}
\end{tabular}
\end{center}